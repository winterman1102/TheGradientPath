# TheGradientPath - Repository Flow Documentation

## ğŸ“‹ Overview

**TheGradientPath** is a comprehensive educational repository covering modern Machine Learning, Deep Learning, and AI from fundamentals to production systems. This document outlines the complete flow and architecture of all projects within this repository.

---

## ğŸ—ï¸ Repository Architecture

```
TheGradientPath/
â”œâ”€â”€ Root Level Scripts          # Quick-start tutorials
â”œâ”€â”€ AiAgents/                   # AI Agent frameworks & benchmarks
â”œâ”€â”€ Keras/                      # Deep Learning with TensorFlow/Keras
â”œâ”€â”€ Pytotch/                    # PyTorch implementations
â”œâ”€â”€ LLMFineTuning/              # LLM fine-tuning techniques
â”œâ”€â”€ Rag/                        # RAG systems & retrieval methods
â”œâ”€â”€ MCPFromScratch/             # Model Context Protocol implementation
â”œâ”€â”€ Text2SQL/                   # Natural language to SQL
â””â”€â”€ RealWorldProjects/          # Production-ready deployments
```

---

## ğŸ”„ Project Flows by Category

### 1. ğŸ¤– AI Agents (`AiAgents/`)

#### **Agent Framework Benchmark Flow**

**Purpose**: Compare 7 major AI agent frameworks using identical multi-agent systems

**Architecture Flow**:
```
User Query
    â†“
Orchestrator Agent (Routing Decision)
    â†“
    â”œâ”€â”€ Legal Expert Agent â†’ Legal domain questions
    â”œâ”€â”€ Operational Agent â†’ Programming & general queries
    â””â”€â”€ Tool Integration
            â†“
        â”œâ”€â”€ Weather API
        â”œâ”€â”€ Calculator
        â”œâ”€â”€ Web Search
        â””â”€â”€ MCP Server Integration
```

**Frameworks Evaluated**:
1. **LangChain/LangGraph** - State machine architecture, maximum flexibility
2. **OpenAI Agents** - Native MCP support, minimal code
3. **CrewAI** - Simple delegation, rapid prototyping
4. **LlamaIndex** - Balanced workflow architecture
5. **AutoGen** - Enterprise async infrastructure
6. **Semantic Kernel** - Microsoft ecosystem integration
7. **Vanilla Python** - Zero framework baseline

**Key Components**:
- Agent orchestration & routing
- Tool integration (custom + built-in)
- State management across agents
- Memory management (conversation history)
- MCP server integration
- Token tracking & guardrails

**Evaluation Metrics**:
- Code complexity & readability
- Developer experience
- Documentation quality
- Feature completeness
- Setup complexity
- Flexibility & abstraction level

---

### 2. ğŸ§  Deep Learning with Keras (`Keras/`)

#### **Image Classification with MLP Flow**

**Dataset**: MNIST handwritten digits

**Pipeline Flow**:
```
Raw MNIST Images (28x28 grayscale)
    â†“
Normalization & Preprocessing
    â†“
Multi-Layer Perceptron
    â”œâ”€â”€ Dense layers
    â”œâ”€â”€ Dropout regularization
    â””â”€â”€ BatchNormalization
    â†“
Softmax Classification (10 classes)
    â†“
TensorBoard Logging & Visualization
```

**Key Features**:
- Functional API architecture
- Visualkeras diagram generation
- Comprehensive metrics tracking

---

#### **Transformer-Based Text Generation Flow**

**Path**: `Keras/transformers/text_generation/`

**Architecture Flow**:
```
Text Input
    â†“
Tokenization & Preprocessing
    â†“
Embedding Layer + Positional Encoding
    â†“
Multi-Head Self-Attention
    â”œâ”€â”€ Query, Key, Value projections
    â”œâ”€â”€ Attention scores computation
    â””â”€â”€ Multi-head concatenation
    â†“
Feed-Forward Network
    â”œâ”€â”€ Dense(hidden_dim)
    â”œâ”€â”€ ReLU activation
    â””â”€â”€ Dense(embed_dim)
    â†“
Layer Normalization + Residual Connections
    â†“
Output Projection Layer
    â†“
Text Generation (token sampling)
```

**Components**:
- Complete Transformer from scratch
- Custom training loop
- Text preprocessing pipeline
- Generation sampling strategies

---

#### **Text Generation with KV Cache Flow**

**Path**: `Keras/transformers/kv_cache_for text_gen/`

**Optimization Flow**:
```
Generation Loop (Token-by-token)
    â†“
Standard Transformer          KV Cache Transformer
    â”œâ”€â”€ Recompute all K,V    â†’    â”œâ”€â”€ Reuse cached K,V
    â”œâ”€â”€ O(nÂ²) complexity     â†’    â”œâ”€â”€ O(n) complexity
    â””â”€â”€ Slow inference       â†’    â””â”€â”€ Fast inference
    â†“
Cached Key-Value States
    â”œâ”€â”€ Store previous computations
    â”œâ”€â”€ Append new K,V pairs
    â””â”€â”€ Dramatically reduced computation
```

**Performance Benefits**:
- Reduced inference latency
- Lower memory overhead during generation
- Production-ready LLM optimization

---

#### **Time Series Forecasting Flow**

**Path**: `Keras/transformers/time_series_forecast/`

**Pipeline Flow**:
```
Synthetic Stock Price Data
    â†“
MinMax Scaling (0-1 normalization)
    â†“
Sequence Creation (sliding window)
    â†“
Transformer Architecture
    â”œâ”€â”€ Temporal embeddings
    â”œâ”€â”€ Multi-head attention (temporal dependencies)
    â””â”€â”€ Feed-forward network
    â†“
Price Prediction (future values)
    â†“
Inverse scaling & Visualization
```

---

### 3. ğŸ”¥ PyTorch Projects (`Pytotch/`)

#### **CNN Image Classification Flow**

**Dataset**: Fashion-MNIST (70,000 images, 10 clothing categories)

**Training Pipeline**:
```
Fashion-MNIST Dataset (28x28 â†’ resize to 16x16)
    â†“
Data Augmentation (optional)
    â†“
CNN Architecture
    â”œâ”€â”€ Conv2d(1â†’16) + BatchNorm + ReLU + MaxPool
    â”œâ”€â”€ Conv2d(16â†’32) + BatchNorm + ReLU + MaxPool
    â””â”€â”€ Fully Connected(512â†’10) + BatchNorm
    â†“
Cross-Entropy Loss + SGD Optimizer
    â†“
Training Loop (with validation)
    â†“
Model Checkpoint (fashion_mnist_cnn.pth)
    â†“
Inference & Evaluation
```

**Key Features**:
- Batch normalization for stability
- Proper train/validation split
- Model persistence & loading
- Real-time training metrics

---

### 4. ğŸ¯ LLM Fine-Tuning (`LLMFineTuning/`)

#### **All PEFT Techniques from Scratch**

**Path**: `LLMFineTuning/all_peft_tecniques_from_scratch/`

**Implementation Flow**:
```
Base Transformer Model
    â†“
PEFT Technique Selection
    â”œâ”€â”€ LoRA (Low-Rank Adaptation)
    â”œâ”€â”€ Prefix Tuning
    â”œâ”€â”€ P-Tuning
    â”œâ”€â”€ Adapter Layers
    â””â”€â”€ BitFit
    â†“
Parameter-Efficient Training
    â”œâ”€â”€ Freeze base model weights
    â”œâ”€â”€ Train only adapter parameters
    â””â”€â”€ Minimal memory footprint
    â†“
Fine-tuned Model (small delta weights)
    â†“
Inference with merged weights
```

**PEFT Techniques Covered**:
- **LoRA**: Low-rank matrices for weight updates
- **Prefix Tuning**: Learnable prefix tokens
- **Adapter Layers**: Small bottleneck layers
- **BitFit**: Bias-only fine-tuning

---

#### **GRPO Reasoning with Unsloth**

**Path**: `LLMFineTuning/GRPO_REASONING_UNSLOTH/`

**Training Flow**:
```
Reasoning Dataset
    â†“
Unsloth Optimization
    â”œâ”€â”€ 2x faster training
    â”œâ”€â”€ 50% memory reduction
    â””â”€â”€ Flash Attention 2
    â†“
GRPO (Group Relative Policy Optimization)
    â”œâ”€â”€ Reward modeling
    â”œâ”€â”€ Policy gradient optimization
    â””â”€â”€ Reasoning capability enhancement
    â†“
Fine-tuned Reasoning Model
```

**Key Optimizations**:
- Unsloth fast inference engine
- GRPO for improved reasoning
- Memory-efficient training

---

#### **SFT with HuggingFace Tool Choice**

**Path**: `LLMFineTuning/SFT_HF_TOOL_CHOICE/`

**Workflow**:
```
Tool-Calling Dataset Generation
    â†“
Dataset Format (gen_dataset.py)
    â”œâ”€â”€ User queries
    â”œâ”€â”€ Tool definitions
    â””â”€â”€ Expected tool selections
    â†“
Supervised Fine-Tuning (SFT)
    â”œâ”€â”€ HuggingFace Trainer
    â”œâ”€â”€ Tool choice classification
    â””â”€â”€ Parameter-efficient training
    â†“
Tool-Aware Language Model
    â†“
Inference: Auto-select appropriate tools
```

**Root Level Scripts**:
- `llm_class_tutorial.py`: Classification with SmolLM2-135M
- `llm_sft_lora.py`: LoRA fine-tuning implementation

---

### 5. ğŸ“– RAG Systems (`Rag/`)

#### **Dartboard RAG Flow**

**Path**: `Rag/dartboard/`

**Retrieval Pipeline**:
```
User Query
    â†“
Query Embedding
    â†“
Vector Database Search
    â†“
Dartboard Algorithm
    â”œâ”€â”€ Relevance Scoring
    â”œâ”€â”€ Diversity Scoring
    â””â”€â”€ Combined Optimization
        â†“
        Select top-k documents
        â”œâ”€â”€ Balance relevance vs diversity
        â”œâ”€â”€ Avoid redundant information
        â””â”€â”€ Maximize information gain
    â†“
Retrieved Documents (non-redundant)
    â†“
LLM Context Injection
    â†“
Generated Answer
```

**Key Components**:
- `ingestion.py`: Document embedding & vector store creation
- `retrieval.py`: Dartboard algorithm implementation
- `main.py`: End-to-end pipeline orchestration

**Algorithm**:
- Optimize: `Score = Î± * Relevance + Î² * Diversity`
- Prevents redundant document retrieval
- Configurable relevance/diversity weights

---

#### **Hybrid Multivector Knowledge Graph RAG**

**Path**: `Rag/hybrid_multivector_knowledge_graph_rag/`

**Advanced Retrieval Flow**:
```
Document Ingestion
    â†“
    â”œâ”€â”€ Text Chunking
    â”œâ”€â”€ Entity Extraction (LLM-powered)
    â”œâ”€â”€ Relationship Extraction
    â””â”€â”€ Multi-Vector Embeddings
        â†“
Knowledge Graph Construction (Neo4j)
    â”œâ”€â”€ Entity nodes
    â”œâ”€â”€ Relationship edges
    â””â”€â”€ Document chunks with embeddings
        â†“
Query Processing
    â”œâ”€â”€ Entity recognition in query
    â”œâ”€â”€ Graph traversal algorithm selection
    â””â”€â”€ LLM-powered Cypher generation
        â†“
Hybrid Retrieval
    â”œâ”€â”€ Vector Similarity Search
    â”œâ”€â”€ Graph Traversal (BFS, DFS, A*, Beam Search)
    â”œâ”€â”€ Causal Chain Discovery
    â””â”€â”€ Multi-hop Reasoning
        â†“
Context Aggregation
    â†“
LLM Generation with Graph Context
```

**11+ Graph Traversal Algorithms**:
1. **BFS** (Breadth-First Search)
2. **DFS** (Depth-First Search)
3. **A*** (Heuristic pathfinding)
4. **Beam Search** (Top-k expansion)
5. **Context-to-Cypher** (LLM query generation)
6. **Causal Chain Traversal**
7. **Similarity-based expansion**
8. **Multi-hop reasoning**
9. **Entity-centric retrieval**
10. **Relationship filtering**
11. **Hybrid scoring**

**Components**:
- `ingestion.py`: Graph construction & embedding
- `query.py`: Hybrid retrieval implementation
- `traversal/`: Graph algorithm implementations
- `prompts.py`: LLM prompt templates

---

#### **Vision RAG**

**Path**: `Rag/vision_rag/`

**Multimodal RAG Flow**:
```
PDF Documents with Images
    â†“
PDF Processing (pdf_processor.py)
    â”œâ”€â”€ Text extraction
    â”œâ”€â”€ Image extraction
    â””â”€â”€ Layout analysis
    â†“
Multimodal Embeddings
    â”œâ”€â”€ Text: OpenAI text embeddings
    â””â”€â”€ Images: CLIP/vision embeddings
    â†“
PostgreSQL + pgvector Storage
    â”œâ”€â”€ Document metadata
    â”œâ”€â”€ Text embeddings
    â””â”€â”€ Image embeddings
    â†“
Query Processing
    â”œâ”€â”€ Text query embedding
    â”œâ”€â”€ Image query embedding (if applicable)
    â””â”€â”€ Hybrid similarity search
    â†“
Retrieved Context (text + images)
    â†“
Vision-Language Model (VLM)
    â†“
Multimodal Answer Generation
```

**Infrastructure**:
- Docker Compose setup (PostgreSQL + pgvector)
- `init_db.py`: Database initialization
- `ingestion.py`: Document processing pipeline
- `query.py`: Multimodal retrieval

---

### 6. ğŸ”Œ MCP From Scratch (`MCPFromScratch/`)

**Model Context Protocol Implementation**

**System Architecture Flow**:
```
Client Application
    â†“
WebSocket Connection
    â†“
MCP Protocol Handshake
    â”œâ”€â”€ Capabilities negotiation
    â”œâ”€â”€ Tool discovery
    â””â”€â”€ Resource listing
    â†“
MCP Server (FastAPI)
    â”œâ”€â”€ Tool Endpoints
    â”‚   â”œâ”€â”€ Calculator
    â”‚   â”œâ”€â”€ Database queries
    â”‚   â””â”€â”€ Text-to-SQL
    â”œâ”€â”€ Prompt Templates
    â””â”€â”€ Resource Access
        â†“
Session Management (SQLite)
    â”œâ”€â”€ API key validation
    â”œâ”€â”€ Usage tracking
    â””â”€â”€ Quota enforcement
        â†“
Tool Execution
    â†“
Response to Client
    â†“
LLM Integration (Client-side)
    â”œâ”€â”€ Tool selection
    â”œâ”€â”€ Parameter extraction
    â””â”€â”€ Natural language response
```

**Components**:

**Server** (`server/`):
- `server.py`: FastAPI WebSocket server
- `server_session.py`: Session management
- `base_session.py`: Protocol base classes
- `protocol_types.py`: Pydantic models
- `initdb.py`: Database initialization

**Client** (`client/`):
- `main_agent.py`: LLM-powered agent
- `main_basic.py`: Simple client example
- `protocol_types.py`: Client-side models

**Key Concepts**:
- Custom protocol design
- WebSocket persistent connections
- Async/await patterns
- API authentication & quotas
- LLM tool integration

---

### 7. ğŸ—ƒï¸ Text2SQL (`Text2SQL/`)

#### **Semantic Text2SQL Flow**

**Path**: `Text2SQL/SemanticText2SQL/`

**Query Processing Pipeline**:
```
Natural Language Query
    â†“
Query Embedding
    â†“
Schema Embedding Database
    â”œâ”€â”€ Table embeddings
    â”œâ”€â”€ Column embeddings
    â””â”€â”€ Relationship embeddings
    â†“
Semantic Similarity Search
    â”œâ”€â”€ Retrieve relevant tables
    â”œâ”€â”€ Retrieve relevant columns
    â””â”€â”€ Retrieve join relationships
    â†“
Schema Context Construction
    â†“
LLM SQL Generation
    â”œâ”€â”€ Schema-aware prompt
    â”œâ”€â”€ Few-shot examples
    â””â”€â”€ SQL syntax validation
    â†“
Generated SQL Query
    â†“
Database Execution (PostgreSQL)
    â†“
Result Formatting & Response
```

**Components**:
- `gen_embeddings.py`: Schema embedding generation
- `db_schema.txt`: Database schema definition
- Docker Compose: PostgreSQL setup
- `init-db.sql`: Database initialization

**Key Features**:
- Schema-aware embedding search
- Context-based SQL generation
- Support for complex joins
- Semantic understanding of queries

---

### 8. ğŸŒ Real-World Production Projects (`RealWorldProjects/`)

#### **Cyber Attack Prediction System**

**Path**: `RealWorldProjects/CyberAttackPrediction/`

**End-to-End Production Flow**:
```
Network Traffic Capture (Scapy)
    â†“
monitor-app (Next.js + Network Agent)
    â”œâ”€â”€ Packet capture
    â”œâ”€â”€ Feature extraction
    â”‚   â”œâ”€â”€ Protocol type
    â”‚   â”œâ”€â”€ Packet size
    â”‚   â”œâ”€â”€ Flow duration
    â”‚   â””â”€â”€ Port numbers
    â””â”€â”€ HTTP POST to ml-service
        â†“
ml-service (Flask API)
    â”œâ”€â”€ Feature preprocessing
    â”œâ”€â”€ AutoEncoder (Anomaly Detection)
    â”‚   â”œâ”€â”€ Normal traffic reconstruction
    â”‚   â””â”€â”€ High error = anomaly
    â””â”€â”€ SGD Classifier (Attack Type)
        â”œâ”€â”€ DDoS
        â”œâ”€â”€ Port Scan
        â”œâ”€â”€ Intrusion
        â””â”€â”€ Benign
        â†“
Prediction Response
    â†“
Web Dashboard (Next.js UI)
    â”œâ”€â”€ Real-time metrics
    â”œâ”€â”€ Attack visualization
    â””â”€â”€ Alert management
```

**AWS Infrastructure (CloudFormation)**:
```
GitHub Repository
    â†“
CodePipeline (CI/CD)
    â”œâ”€â”€ Source Stage (GitHub)
    â”œâ”€â”€ Build Stage (CodeBuild)
    â””â”€â”€ Deploy Stage (CodeDeploy)
        â†“
Application Load Balancer
    â”œâ”€â”€ HTTPS listener (port 443)
    â””â”€â”€ HTTP listener (port 80)
        â†“
Auto Scaling Groups
    â”œâ”€â”€ monitor-app instances
    â””â”€â”€ ml-service instances
        â†“
Target Groups (Health Checks)
    â†“
Production Traffic Handling
```

**Infrastructure Components**:
- **CF_NETWORK_ATTACK_PREDICTION.yml**: Complete CloudFormation template
- **Auto Scaling**: Dynamic capacity based on traffic
- **Load Balancing**: High availability
- **CI/CD**: Automated deployments
- **Security Groups**: Network isolation
- **IAM Roles**: Secure permissions

**Deployment Flow**:
1. Push code to GitHub
2. CodePipeline triggers automatically
3. CodeBuild compiles and tests
4. CodeDeploy deploys to EC2 instances
5. Health checks ensure successful deployment
6. Load balancer routes traffic to new instances

---

## ğŸ”— Cross-Project Integration Patterns

### **Common Architectural Patterns**

1. **Embedding-Based Retrieval**
   - Used in: RAG systems, Text2SQL, Vision RAG
   - Flow: Text/Image â†’ Embedding â†’ Vector DB â†’ Similarity Search

2. **LLM Agent Orchestration**
   - Used in: AI Agents, MCP, Tool-calling LLMs
   - Flow: User Query â†’ Agent Routing â†’ Tool Selection â†’ Execution

3. **Multi-Stage Pipelines**
   - Used in: All projects
   - Flow: Input â†’ Preprocessing â†’ Model Inference â†’ Post-processing â†’ Output

4. **Production Deployment**
   - Used in: Cyber Attack Prediction
   - Flow: GitHub â†’ CI/CD â†’ Build â†’ Test â†’ Deploy â†’ Monitor

---

## ğŸ› ï¸ Technology Stack Overview

### **Frameworks & Libraries**
- **Deep Learning**: TensorFlow/Keras, PyTorch
- **LLM**: HuggingFace Transformers, Unsloth, OpenAI API
- **Agent Frameworks**: LangChain, CrewAI, AutoGen, LlamaIndex, Semantic Kernel
- **Vector DBs**: ChromaDB, pgvector, Neo4j
- **Web**: FastAPI, Next.js, Flask
- **Deployment**: AWS (CloudFormation, CodePipeline, EC2, ALB)

### **Key Techniques**
- **Fine-tuning**: LoRA, PEFT, SFT, GRPO
- **RAG**: Vector search, Graph traversal, Dartboard algorithm
- **Attention**: Multi-head self-attention, KV cache
- **Optimization**: Batch normalization, Dropout, Early stopping
- **ML**: CNNs, Transformers, AutoEncoders, Classification

---

## ğŸ“Š Learning Path Recommendation

### **Beginner Path**
1. `Pytotch/CnnImageClassification/` - Basic CNN concepts
2. `Keras/ImageClassificationWithMLP/` - Neural network fundamentals
3. `llm_class_tutorial.py` - LLM classification basics

### **Intermediate Path**
1. `Keras/transformers/text_generation/` - Transformer architecture
2. `Rag/dartboard/` - RAG fundamentals
3. `LLMFineTuning/SFT_HF_TOOL_CHOICE/` - Fine-tuning techniques
4. `MCPFromScratch/` - Client-server architecture

### **Advanced Path**
1. `AiAgents/AgentFrameworkBenchmark/` - Multi-agent systems
2. `Rag/hybrid_multivector_knowledge_graph_rag/` - Advanced retrieval
3. `LLMFineTuning/GRPO_REASONING_UNSLOTH/` - Advanced fine-tuning
4. `RealWorldProjects/CyberAttackPrediction/` - Production deployment

---

## ğŸ¯ Use This Document For

- **Understanding project architecture** before diving into code
- **Planning learning paths** based on skill level
- **Identifying relevant components** for specific use cases
- **Prompting LLMs** with complete context of repository structure
- **Onboarding new contributors** to the repository
- **Designing similar systems** using proven patterns

---

## ğŸ“ Quick Reference

### **Project Selection Guide**

| Goal | Recommended Project |
|------|-------------------|
| Learn Transformers from scratch | `Keras/transformers/text_generation/` |
| Build production RAG | `Rag/hybrid_multivector_knowledge_graph_rag/` |
| Compare agent frameworks | `AiAgents/AgentFrameworkBenchmark/` |
| Deploy ML to AWS | `RealWorldProjects/CyberAttackPrediction/` |
| Understand PEFT | `LLMFineTuning/all_peft_tecniques_from_scratch/` |
| Build custom protocol | `MCPFromScratch/` |
| Optimize LLM inference | `Keras/transformers/kv_cache_for text_gen/` |
| Learn graph-based retrieval | `Rag/hybrid_multivector_knowledge_graph_rag/` |

---

## ğŸ”„ Continuous Updates

This repository is actively maintained with:
- **Video tutorials** for major projects (YouTube links in READMEs)
- **Jupyter notebooks** for interactive learning
- **Production-ready code** with comprehensive documentation
- **Regular updates** with latest techniques and frameworks

---

**Last Updated**: November 2025
**Maintained By**: TheGradientPath Community
